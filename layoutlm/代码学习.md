## 数据

### 数据准备

原始的XFUND 数据集，PaddleOCR提供了转换脚本，可以将原始数据集转换为PaddleOCR所需的数据格式。

手动准备数据，需要满足PaddleOCR需要的标注格式

```bash
wget https://paddleocr.bj.bcebos.com/dataset/XFUND.tar
tar -xf XFUND.tar

# XFUN其他数据集使用下面的代码进行转换
# 代码链接：https://github.com/PaddlePaddle/PaddleOCR/blob/release%2F2.4/ppstructure/vqa/helper/trans_xfun_data.py

python3 ppstructure/vqa/tools/trans_xfun_data.py --ori_gt_path=path/to/json_path --output_path=path/to/save_path
```

### 标注格式

```txt
"图像文件名    图像标注信息"
zh_train_0.jpg   [{}, {}, {}]
```

**注意：** 文本文件中默认请将图片路径和图片标签用 `\t` 分割。

```json
[
    {
        "transcription": "邮政地址:",
        "label": "question",
        "points": [[261, 802], [483, 802], [483, 859], [261, 859]],
        "id": 54,
        "linking": [[54, 60]]
    },
    {}
]
```

其中图像标注信息字符串经过`json`解析之后可以得到一个列表信息，列表中每个元素是一个字典，存储了每个文本行的需要信息，各个字段的含义如下。

- `transcription`: 存储了文本行的文字内容
- `label`: 该文本行内容所属的类别
- `points`: 存储文本行的四点位置信息，（左上角，右上角，右下角，左下角）
- `id`: 存储文本行的id信息，用于RE任务的训练
- `linking`: 存储文本行的之间的连接信息，用于RE任务的训练

```json
{
    "ocr_info": [
        {
            "text": "邮政地址:",  # 单个文本内容
            "label": "question", # 文本所属类别
            "bbox": [261, 802, 483, 859], # 单个文本框
            "id": 54,  # 文本索引
            "linking": [[54, 60]], # 当前文本和其他文本的关系 [question, answer]
            "words": []
        },
        {
            "text": "湖南省怀化市市辖区",
            "label": "answer",
            "bbox": [487, 810, 862, 859],
            "id": 60,
            "linking": [[54, 60]],
            "words": []
        }
    ]
}

```

### 字典文件

训练集与验证集中的文本行包含标签信息，所有标签的列表存在字典文件中（如`class_list.txt`），字典文件中的每一行表示为一个类别名称。

以`XFUND_zh`数据为例，共包含4个类别，字典文件内容如下所示。

```
OTHER
QUESTION
ANSWER
HEADER
```

**注：**

- 标注文件中的类别信息不区分大小写，如`HEADER`与`header`会被解析为相同的类别id，因此在标注的时候，不能使用小写处理后相同的字符串表示不同的类别。
- 在整理标注文件的时候，建议将`other`这个类别（其他，无需关注的文本行可以标注为other）放在第一行，在解析的时候，会将`other`类别的类别id解析为0，后续不会对该类进行可视化。


## SER

### 损失函数

配置文件

```yml
Loss:
  name: VQASerTokenLayoutLMLoss
  num_classes: *num_classes
  key: "backbone_out"
```

`ppocr/losses/vqa_token_layoutlm_loss.py`

```python
class VQASerTokenLayoutLMLoss(nn.Layer):
    def __init__(self, num_classes, key=None):
        super().__init__()
        self.loss_class = nn.CrossEntropyLoss()
        self.num_classes = num_classes
        self.ignore_index = self.loss_class.ignore_index
        self.key = key

    def forward(self, predicts, batch):
        if isinstance(predicts, dict) and self.key is not None:
            predicts = predicts[self.key]
        labels = batch[5]
        attention_mask = batch[2]
        if attention_mask is not None:
            active_loss = attention_mask.reshape([-1, ]) == 1
            active_output = predicts.reshape(
                [-1, self.num_classes])[active_loss]
            active_label = labels.reshape([-1, ])[active_loss]
            loss = self.loss_class(active_output, active_label)
        else:
            loss = self.loss_class(
                predicts.reshape([-1, self.num_classes]),
                labels.reshape([-1, ]))
        return {'loss': loss}
```

这里的`predicts`是模型的输出结果，`batch`是模型的输入。

```python
preds = model(batch)
```

损失实在 `train`训练循环中计算的。


### 数据集加载

#### transforms 预处理变换

初始输入

```python
{
    "img_path": "test.jpg",
    "label": ocr_info,
    "img": img
}
```


**`DecodeImage`**

```yml
- DecodeImage: # load image
    img_mode: RGB
    channel_first: False
```

输出

```python
{
    "img_path": "test.jpg",
    "label": ocr_info,
    "img": img
}
```

**`VQATokenLabelEncode`**

```yml
- VQATokenLabelEncode: # Class handling label
    contains_re: False
    algorithm: *algorithm
    class_path: *class_path
    use_textline_bbox_info: &use_textline_bbox_info True
    # one of [None, "tb-yx"]
    order_method: &order_method "tb-yx"
```

输出：

```python
{
    'img_path': str, 
    'image': np.array, 
    'ocr_info': list, # 标注格式的列表
    'bbox': list, # [x1, y1, x2, y2]
    'labels': list, 
    'input_ids': list, # token 级别
    'token_type_ids': list, # 是否是视觉token
    'attention_mask': list, 
    'segment_offset_id': list, # 当前tokens在哪个segment中，哪个OCR 块block中 
    'tokenizer_params': dict, 
    'entities': list
} 
```

对于token的编码，是遍历 ocr_info 中的每个元素，然后对每个元素进行编码，形成一个**列表**。每个 token绑定一个 bbox。

`tokenizer`:

input_ids: 表示输入文本的token ID。
token_type_ids: 表示对应的token属于输入的第一个句子还是第二个句子。（Transformer类预训练模型支持单句以及句对输入。）
attention_mask: 

```python
input_ids: [0, 6, 206691, 66219, 5873]
token_type_ids: [0, 0, 0, 0, 0]
attention_mask: [1, 1, 1, 1, 1]
```

`entities` 是 每个token 的列表开始位置，以及结束位置, 标签

```python
{
    "start": len(input_ids_list),
    "end": len(input_ids_list) + len(encode_res["input_ids"]),
    "label": 'O',
}
```

`VQATokenPad`

```yml
- VQATokenPad:
    max_seq_len: &max_seq_len 512
    return_attention_mask: True
```

Pad 后转化为 tensor，同时截断序列

```python
{
    'img_path': str, 
    'image': np.array, 
    'ocr_info': list, # 标注格式的列表
    'input_ids': list, # token 级别
    'labels': list, 
    'token_type_ids': list, # 是否是视觉token
    'bbox': list, # [x1, y1, x2, y2]
    'attention_mask': list, 
    'segment_offset_id': list, # 当前tokens在哪个segment中，哪个OCR 块block中 
    'entities': list
} 
```

`VQASerTokenChunk`

```yml
- VQASerTokenChunk:
    max_seq_len: 512
```

```python
{
    'img_path': str, 
    'image': np.array, 
    'ocr_info': list, # 标注格式的列表
    'input_ids': list, # token 级别
    'labels': list, 
    'token_type_ids': list, # 是否是视觉token
    'bbox': list, # [x1, y1, x2, y2]
    'attention_mask': list, 
    'segment_offset_id': list, # 当前tokens在哪个segment中，哪个OCR 块block中 
    'entities': list
}
```

`Resize`

```yml
- Resize:
    size: [224, 224]
```

`NormalizeImage`

```yml
- NormalizeImage:
    scale: 1
    mean: [123.675, 116.28, 103.53]
    std: [58.395, 57.12, 57.375]
    order: "hwc"
```

`ToCHWImage`

```yml
- ToCHWImage:
```    

`KeepKeys`

```yml
- KeepKeys:
    keep_keys: [
        "input_ids",
        "bbox",
        "attention_mask",
        "token_type_ids",
        "image",
        "labels",
    ] # dataloader will return list in this order
```

实际：

```python
['input_ids', 'bbox', 'attention_mask', 'token_type_ids', 'image', 'labels', 'segment_offset_id', 'ocr_info', 'entities']
```

#### 训练阶段

配置文件

```yaml
Train:
  dataset:
    name: SimpleDataSet
    data_dir: train_data/union/image
    label_file_list:
      - train_data/union/train.json
    ratio_list: [1.0]
    transforms:
        # ...  
    
  loader:
    shuffle: True
    drop_last: False
    batch_size_per_card: 8
    num_workers: 8
```

关注 `class SimpleDataset` 中的 `__getitem__` 方法

### 训练模型

#### 模型加载

配置文件

```yml
Architecture:
  model_type: kie
  algorithm: "LayoutXLM"
  Transform:
  Backbone:
    name: LayoutXLMForSer
    pretrained: True
    checkpoints:
    # one of base or vi
    mode: vi # 加载预训练模型权重
    num_classes: 7
```

`class LayoutXLMForSer(NLPBaseModel)` 
文件路径 `ppocr/modeling/backbones/vqa_layoutlm.py`

预训练模型权重加载：

```python
pretrained_model_dict = {
    LayoutXLMModel: {
        "base": "layoutxlm-base-uncased",
        "vi": "vi-layoutxlm-base-uncased", # 不适用视觉后端
    }
}
```

`PaddleNLP`

`paddlenlp/transformers/model_utils.py` 文件

```python
class PretrainedModel(Layer, GenerationMixin):
    # ...

    def from_pretrained()
        # ....
        # 加载当前类定义的预训练权重
        elif pretrained_model_name_or_path in cls.pretrained_init_configuration:
        # ...
```

定位到 `class LayoutXLMPretrainedModel(PretrainedModel)`，文件路径 `paddlenlp/transformers/layoutxlm/modeling.py` 模型定义。


### LayoutXLM模型

```python
x = self.model(
    input_ids=x[0],
    bbox=x[1],
    attention_mask=x[2],
    token_type_ids=x[3],
    image=image,
    position_ids=None,
    head_mask=None,
    labels=None)
if self.training:
    res = {"backbone_out": x[0]}
    res.update(x[1])
    return res
else:
    return x

# model 是 LayoutXLMForTokenClassification
# 首先会进入 LayoutXLMModel进行编码
```

`LayoutXLMModel` 输出

输出

```python
encoder_outputs = self.encoder(
    final_emb,
    extended_attention_mask,
    bbox=final_bbox,
    position_ids=final_position_ids,
    head_mask=head_mask,
    output_attentions=output_attentions,
    output_hidden_states=output_hidden_states,
)
sequence_output = encoder_outputs[0]
pooled_output = self.pooler(sequence_output)
return sequence_output, pooled_output, encoder_outputs[1]
```

其中 `encoder_outputs: (tensor, dict)` 为编码器输出

```python
sequence_output: [batch_size, seq_len, 768]
pooled_output: [batch_size, 768]
encoder_outputs[1]: dict_keys(['input_hidden_states', 'input_attention_mask', 'input_layer_head_mask', '0_data', '1_data', '2_data', '3_data', '4_data', '5_data', '6_data', '7_data', '8_data', '9_data', '10_data', '11_data'])
```

`{idx}_data` 为隐藏层的输出，`shape` 为 `[batch_size, seq_len, 768]`


`LayoutXLMForTokenClassification` 输出

```python
if self.training:
    outputs = logits, hidden_states
else:
    outputs = logits

# logits: [batch_size, seq_len, num_labels]
# hidden_states: dict('hidden_states_{idx}') 隐藏层输出
```

最后 `LayoutXLMForSer` 输出


```python
if self.training:
    res = {"backbone_out": x[0]}
    res.update(x[1])
    return res
else:
    return x
```

```python
{
    'backbone_out': [batch_size, 512, num_labels],
    'hidden_states_0': [batch_size, seq_len, 768],
    'hidden_states_1': [batch_size, seq_len, 768],
    # ...
}
```

```
[batch_size, seq_len, num_labels]
```

后处理，添加类别

类别数目：7= (B,I)*3 + O

模型在这个文件：`vqa_layoutlm.py`


### 推理 

`infer_kie_token_ser_re.py`

```python
ser_results, ser_inputs = self.ser_engine(data)

# ser_results
ocr_infos = dict{
    "pred_id": pred_id,
    "pred": pred,
    ...
}
# ser_inputs
# ['keep_keys'] = 
[
    'input_ids', 'bbox', 'attention_mask', 'token_type_ids',
    'image', 'labels', 'segment_offset_id', 'ocr_info',
    'entities'
]
```



## OCR 获得 ocr_info

```python
    ocr 变换

   1. OCR 模型输入图片，得到ocr_info
   ```json
   {
    "transcription": "",
    "bbox": ["x1", "y1", "x2", "y2"],
    "points": [ # 多边形点
        ["x1", "y1"],
        ["x2", "y2"],
        ["x3", "y3"],
        ["x4", "y4"]
    ],
   }
   ```

   2. tokenizer
    for each ocr_info
   {'input_ids': list, 'token_type_ids': list, 'attention_mask': list}
    拼接为一维序列
```